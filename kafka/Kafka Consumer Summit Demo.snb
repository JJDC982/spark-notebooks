{"metadata":{"name":"Kafka Consumer","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/home/maasg/.m2/repository","customRepos":null,"customDeps":["org.apache.spark % spark-streaming-kafka_2.10 % 1.4.1"],"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.streaming._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.streaming._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.SparkConf\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":88}]},{"metadata":{},"cell_type":"markdown","source":"#Recreate the undelying Spark Context\n## Sets config for Spark Streaming"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"StreamingContext.getActive.foreach(_.stop(true))","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":89}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"reset(\"Summit Demo\", lastChanges = (c:SparkConf) => {\n  \n   c.set(\"spark.streaming.blockInterval\", \"1000\")\n    .set(\"spark.master\", \"local[3]\")\n  //.set(\"spark.streaming.receiver.maxRate\", \"50000\")\n})","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":90}]},{"metadata":{},"cell_type":"markdown","source":"#Data Stream Visualization"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val list = ul(10)\nfor (i<- 1 to 10) {list.append(\"...\")}\nlist","outputs":[{"name":"stdout","output_type":"stream","text":"warning: there were 1 feature warning(s); re-run with -feature for details\nlist: notebook.front.DataConnectedWidget[String]{implicit val singleCodec: notebook.Codec[play.api.libs.json.JsValue,String]; def data: Seq[String]; def data_=(x$1: Seq[String]): Unit; lazy val toHtml: scala.xml.Elem; def append(s: String): Unit; def appendAll(s: Seq[String]): Unit} = <anon$1 widget>\nres65: notebook.front.DataConnectedWidget[String]{implicit val singleCodec: notebook.Codec[play.api.libs.json.JsValue,String]; def data: Seq[String]; def data_=(x$1: Seq[String]): Unit; lazy val toHtml: scala.xml.Elem; def append(s: String): Unit; def appendAll(s: Seq[String]): Unit} = <anon$1 widget>\n"},{"metadata":{},"data":{"text/html":"<ul data-bind=\"foreach: value\">\n      <li data-bind=\"text: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anon964a10382daf56d33058d4b482e0bf00&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n          /*]]>*/</script></ul>"},"output_type":"execute_result","execution_count":91}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val debug = ul(10)\nfor (i<- 1 to 10) {debug.append(\"...\")}\ndebug","outputs":[{"name":"stdout","output_type":"stream","text":"warning: there were 1 feature warning(s); re-run with -feature for details\ndebug: notebook.front.DataConnectedWidget[String]{implicit val singleCodec: notebook.Codec[play.api.libs.json.JsValue,String]; def data: Seq[String]; def data_=(x$1: Seq[String]): Unit; lazy val toHtml: scala.xml.Elem; def append(s: String): Unit; def appendAll(s: Seq[String]): Unit} = <anon$1 widget>\nres66: notebook.front.DataConnectedWidget[String]{implicit val singleCodec: notebook.Codec[play.api.libs.json.JsValue,String]; def data: Seq[String]; def data_=(x$1: Seq[String]): Unit; lazy val toHtml: scala.xml.Elem; def append(s: String): Unit; def appendAll(s: Seq[String]): Unit} = <anon$1 widget>\n"},{"metadata":{},"data":{"text/html":"<ul data-bind=\"foreach: value\">\n      <li data-bind=\"text: $data\"></li><script data-this=\"{&quot;valueId&quot;:&quot;anon87afb3d1e247499373acab705bb15a52&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n          /*]]>*/</script></ul>"},"output_type":"execute_result","execution_count":92}]},{"metadata":{},"cell_type":"markdown","source":"#Create the Streaming Context\n## Uses the config provided by the Spark Context"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val streamingContext = new StreamingContext(sparkContext, Seconds(2))","outputs":[{"name":"stdout","output_type":"stream","text":"streamingContext: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@68e0c3f\n"},{"metadata":{},"data":{"text/html":"org.apache.spark.streaming.StreamingContext@68e0c3f"},"output_type":"execute_result","execution_count":93}]},{"metadata":{},"cell_type":"markdown","source":"##Kafka configuration"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val zkQuorum=\"172.17.0.6:2181\"\nval group = \"throughput\"\nval numThreads = 2\nval topics= \"datatopic\"","outputs":[{"name":"stdout","output_type":"stream","text":"zkQuorum: String = 172.17.0.6:2181\ngroup: String = throughput\nnumThreads: Int = 2\ntopics: String = datatopic\n"},{"metadata":{},"data":{"text/html":"datatopic"},"output_type":"execute_result","execution_count":94}]},{"metadata":{},"cell_type":"markdown","source":"## Job Definition "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\nval stream = KafkaUtils.createStream(streamingContext, zkQuorum, group, topicMap).map(_._2)\nstream.foreachRDD{rdd=>\n                  val count = rdd.count()\n                  debug.append(s\"rdd count: $count\")\n                  rdd.take(10).foreach(list.append(_))\n                 }\nval parsedStream = stream.map(line => line.split(\",\"))\n                         .flatMap{case Array(typ, ts, value) => \n                                  try{\n                                     val t = ts.trim.toLong\n                                     val v = value.trim.toDouble\n                                     Some(((typ,t),v))\n                                  } catch {\n                                    case e:NumberFormatException => None\n                                  }\n                                 }\nparsedStream.cache\nval streamSplitter:String => DStream[((String, Long), Double)] => DStream[(Long,Double)] = key => stream => \n  stream.filter(_._1._1 == key).map{case ((typ,ts), v) => (ts,v)}\nval temperature = streamSplitter(\"temperature\")(parsedStream)\nval humidity = streamSplitter(\"humidity\")(parsedStream) \nval pressure = streamSplitter(\"pressure\")(parsedStream)\nval reducedt = temperature.transform{rdd => rdd.groupByKey.mapValues(v => v.sum/v.size)}\nval reducedh = humidity.transform{rdd => rdd.groupByKey.mapValues(v => v.sum/v.size)}\nval corr = reducedt.join(reducedh).mapValues{case (t,h) => Math.sqrt(t*t+h*h)}\n\ncorr.foreachRDD{rdd => \n                     val cnt = rdd.count()\n                     debug.append(s\"corr count: $cnt\")\n                    }","outputs":[{"name":"stdout","output_type":"stream","text":"warning: there were 3 feature warning(s); re-run with -feature for details\ntopicMap: scala.collection.immutable.Map[String,Int] = Map(datatopic -> 2)\nstream: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@7e767b3b\nparsedStream: org.apache.spark.streaming.dstream.DStream[((String, Long), Double)] = org.apache.spark.streaming.dstream.FlatMappedDStream@14949f5a\nstreamSplitter: String => (org.apache.spark.streaming.dstream.DStream[((String, Long), Double)] => org.apache.spark.streaming.dstream.DStream[(Long, Double)]) = <function1>\ntemperature: org.apache.spark.streaming.dstream.DStream[(Long, Double)] = org.apache.spark.streaming.dstream.MappedDStream@2969d844\nhumidity: org.apache.spark.streaming.dstream.DStream[(Long, Double)] = org.apache.spark.streaming.dstream.MappedDStream@63f56d99\npressure: org.apache.s..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":97}]},{"metadata":{},"cell_type":"markdown","source":"#Write data to disk "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"temperature.saveAsTextFiles(\"/tmp/summit\", \"temperature\")\nhumidity.saveAsTextFiles(\"/tmp/summit\", \"humidity\")\npressure.saveAsTextFiles(\"/tmp/summit\", \"pressure\")\ncorr.saveAsTextFiles(\"/tmp/summit\", \"correlation\")","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":98}]},{"metadata":{},"cell_type":"markdown","source":"#Let the ball roll..."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"streamingContext.start()","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":99}]},{"metadata":{},"cell_type":"markdown","source":"[Go up to viz](#Data-Stream-Visualization)"}],"nbformat":4}