{
  "metadata" : {
    "name" : "language-detection-letter-freq",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "6D985D5B5E8740F384AA83EDE40A95D2"
    },
    "cell_type" : "markdown",
    "source" : "#Language Classification\n###A naive approach to language classification.\nThis notebook explores the language classification problem by looking at the frequency distribution of the characters used in sample text.\nUsing the letter frequency we build simple models that will allow us to classify the language of new sentences.\n\nFor our exploration, we will use a dataset comprised of treaties and other official documents of the European Commission. Those are available in each language spoken in the European Union and hence ideal to have a datasets of equivalent quality for each language."
  }, {
    "metadata" : {
      "id" : "5A3E909A40B049FE9CB7DCB3383384CE"
    },
    "cell_type" : "markdown",
    "source" : "## First, we will load some sample data and explore the character distribution in order to build up some intuition."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BFF3A95A5486431F944D5D66AD38CD76"
    },
    "cell_type" : "code",
    "source" : "val notebooksFolder = sys.env(\"NOTEBOOKS_DIR\")\nval baseFolder = s\"$notebooksFolder/languageclassification/data\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C9DB899751B54B4886E418E70D116C0F"
    },
    "cell_type" : "markdown",
    "source" : "### We load the english dataset to explore the data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2A1476EDDD3344B98FEFF9988DB6786C"
    },
    "cell_type" : "code",
    "source" : "val en = sparkSession.sparkContext.textFile(baseFolder + \"/en\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "2FE75BFDD8AC4136827BDCDFFA4AC5E0"
    },
    "cell_type" : "markdown",
    "source" : "### And we clean up the text from characters other than letters"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "41A6E9AA80E24EA2868949ECC6878BFD"
    },
    "cell_type" : "code",
    "source" : "val cleanedLetters = en.flatMap(str => str).filter(java.lang.Character.isAlphabetic(_)).map(java.lang.Character.toLowerCase(_))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "DD074C27F4C24EE88BF7D0849B0AB947"
    },
    "cell_type" : "markdown",
    "source" : "#### We can use cells to interactively test our filter method to be sure we are getting the results that we expect\nThe API parity between Scala and Spark lets us easily tests on local collections. Like a string in this case."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CEBFFF79769A4E77A11BF8DB068454E4"
    },
    "cell_type" : "code",
    "source" : "val sample = \"Erwägung protección jurídica ci-après à l’aide Gemäß 987...\"\nsample.filter(java.lang.Character.isAlphabetic(_)).map(java.lang.Character.toLowerCase(_))\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "36CCEBEC50724BC79349061B101C76D2"
    },
    "cell_type" : "markdown",
    "source" : "### We count the total of characters in our dataset\nWe will need this later to obtain relative frequency values"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DD4F41176D2B4EEFAE850DB5AF815579"
    },
    "cell_type" : "code",
    "source" : "val total = cleanedLetters.count()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "68E13860F02440D29757330CC1EF7AD2"
    },
    "cell_type" : "markdown",
    "source" : "### ...and the frequency of each alphabetic character in the dataset\nNote that the frequency is relative to the total count - this will enable us to compare different languages later on"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6F6D9275B5E54FFAB770F4418BDF2554"
    },
    "cell_type" : "code",
    "source" : "val freq = cleanedLetters.keyBy(char => char.toString.toLowerCase).countByKey.map{case (k,v) => (k.toString, v.toDouble/total)}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "48C9B6C691AC4DCE8DB57DC606463CAB"
    },
    "cell_type" : "markdown",
    "source" : "#### We are interested in the frequency of each letter, alphabetically ordered"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E31D0FA9B28249F08FA7C96F64EB2429"
    },
    "cell_type" : "code",
    "source" : "val freqOrdered = freq.toList.sortBy{case (k,v)=> k}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "6E4B1184392945DAA606D18EF97AC806"
    },
    "cell_type" : "markdown",
    "source" : "### We can now visualize the how the frequency distribution looks like"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1357868862-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "C46268BE6A0D4E1F866D14F4EB18A980"
    },
    "cell_type" : "code",
    "source" : "freqOrdered",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "90FDBDE535424CC0897C20A6989845C3"
    },
    "cell_type" : "markdown",
    "source" : "### Probably better to take the relevant part"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab552129050-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "A780D4906EC84FCD8339A250ADF50EF0"
    },
    "cell_type" : "code",
    "source" : "freqOrdered.take(30)\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1F8D760BD9094A1C8956F0D9EF2AEE8D"
    },
    "cell_type" : "markdown",
    "source" : "## Let's put together these initial steps to process other languages and compare the distributions\nDefine a method `letterFreq` that takes an `RDD[String]` and produces a sequence of pairs of a letter and its relative frequency"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "26429C6A81D24253AF14041AEB781E0F"
    },
    "cell_type" : "code",
    "source" : "import java.lang.Character\nimport org.apache.spark.rdd.RDD\n  \ndef letterFreq(rdd: RDD[String]): Seq[(String, Double)] = {\n  val cleanedChars = rdd.flatMap(str => str).collect{case char if Character.isAlphabetic(char) => Character.toLowerCase(char)}\n  val total = cleanedChars.count\n  val freq = cleanedChars.keyBy(identity).countByKey()\n  // here we transform characters to String to help us with limited support for 'char' in Spark Datasets\n  val ordered = freq.map{case (k,v) => (k.toString, v.toDouble/total)}.toList.sortBy{case (k,v)=> k}\n  ordered\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "74CE994500F4495B9B123B94D6AB99F5"
    },
    "cell_type" : "code",
    "source" : "val es = sparkSession.sparkContext.textFile(baseFolder + \"/es\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E21E00DDADFB43649D682F2E8807DAE4"
    },
    "cell_type" : "code",
    "source" : "val esLetterFreq = letterFreq(es).take(30)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "91F563460CC145CF931BAF324B1BF902"
    },
    "cell_type" : "markdown",
    "source" : "### Let's compare the frequency distribution of English vs Spanish"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "337B9750FD2C4E6880194D2B835B2D8A"
    },
    "cell_type" : "code",
    "source" : "def withLanguage(data: Seq[(String, Double)], lang:String) = data.map{case (letter, freq) => (lang, letter, freq)}\nval esVsEn = withLanguage(esLetterFreq, \"es\") ++ withLanguage(freqOrdered.take(30), \"en\")\nCustomPlotlyChart(esVsEn,\n                  layout=\"{title: 'Language Frequency Distribution Comparison: ES vs EN'}\",\n                  dataOptions=\"\"\"{type: 'line', splitBy: '_1'}\"\"\",\n                  dataSources=\"{x: '_2', y: '_3'}\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3507E4C4387C462EB08775FD2419E945"
    },
    "cell_type" : "markdown",
    "source" : "# Let's create a classifier for 6 common european languages"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C0AA0B4BBA274409867FBF255D234787"
    },
    "cell_type" : "code",
    "source" : "val languages = Seq(\"en\", \"es\", \"it\", \"de\", \"fr\", \"nl\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "393F496631B549A583C26FE8AE7F850B"
    },
    "cell_type" : "code",
    "source" : "val langDataset = languages.map{lang => (lang, sparkContext.textFile(baseFolder + s\"/$lang\"))}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "770E09E738CD472A8047D1F6CBAA32F1"
    },
    "cell_type" : "code",
    "source" : "val langLetterFreq = langDataset.map{case (lang, rdd) => (lang, letterFreq(rdd))}\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "988317526ABC47D3828CC9D10180B436"
    },
    "cell_type" : "markdown",
    "source" : "#### This `case class` helps to provide a schema to the dataset to help us plotting it."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "944D0547A7CD437F97DC6248A324D550"
    },
    "cell_type" : "code",
    "source" : "case class LanguageLetterFrequency(lang: String, letter: String, freq: Double)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CAB89D26080541CA9731BADBC4876794"
    },
    "cell_type" : "code",
    "source" : "val langByLetterFreq = langLetterFreq.flatMap{case (lang, freqList) => freqList.take(30).map{case (letter, freq) => LanguageLetterFrequency(lang, letter, freq)}}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A22112C532A34B759772E92B8770615A"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(langByLetterFreq,\n                  layout=\"{title: 'Language Frequency Distribution Comparison'}\",\n                  dataOptions=\"\"\"{type: 'bar', splitBy: 'lang'}\"\"\",\n                  dataSources=\"{x: 'letter', y: 'freq'}\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D0AFDA1AE6BB40E993942FC46C59FDF1"
    },
    "cell_type" : "markdown",
    "source" : "#Naive Language Prediction"
  }, {
    "metadata" : {
      "id" : "4580665C68B2443F8C3386112625FA93"
    },
    "cell_type" : "markdown",
    "source" : "## We first need to apply the same process to the text to evaluate\nAgain, API parity between Spark and Scala to the rescue"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9657B6F72C124AB1A7A37F65B64A1E08"
    },
    "cell_type" : "code",
    "source" : "def sentenceFreq(str: String):List[(String, Double)] = {\n  val cleaned = str.collect{case char if Character.isAlphabetic(char) => Character.toLowerCase(char)}\n  val total = cleaned.size\n  val freq = cleaned.groupBy(identity).map{case (letter, group) => (letter, group.size)}\n  val ordered = freq.map{case (k,v) => (k.toString, v.toDouble/total)}.toList.sortBy{case (k,v)=> k}\n  ordered\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab170395340-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "CD5EF3BF238040108CBE4116F3F1ACBB"
    },
    "cell_type" : "code",
    "source" : "sentenceFreq(\"estamos haciendo un modelo para prediccion de lenguage\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B7E80377289A454E8265A0FF3B9CBA5B"
    },
    "cell_type" : "markdown",
    "source" : "## Our classifier will use Euclidean distance between the sample and each of our pre-calculated models\nThe closer we are from a model, the higher likehood that we have a match "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B2E891AEBDF45B586683CB38E20464A"
    },
    "cell_type" : "code",
    "source" : "def classify(str:String): Seq[(String, Double)] = {\n  val strFreq = sentenceFreq(str).toMap\n  langLetterFreq.map{case (lang, refFreq) => \n                     val freqMap = refFreq.toMap\n                     val score = strFreq.map{case (letter, freq) => \n                                 val modelFreq = freqMap.get(letter)\n                                 modelFreq.map(mf => (freq - mf)*(freq-mf)).getOrElse(1.0)\n                                }.sum\n                     (lang, Math.sqrt(score))\n                    }.sortBy(_._2)\n}\n  ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab526730756-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "98C383AFE06C413DB9C5C354F8E6E73D"
    },
    "cell_type" : "code",
    "source" : "classify(\"è\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab761691932-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "CBCB400234D543D286DFFBEF8094D4F5"
    },
    "cell_type" : "code",
    "source" : "classify(\"estamos haciendo un modelo para clasificar el lenguage usado en un documento\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1993981453-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "D897ABDAB4BB40CD81B9ABE9C3D19C97"
    },
    "cell_type" : "code",
    "source" : "classify(\"wij maken een model om de taal te bepalen die in een document gebruikt wordt\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab426369280-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E5C4B5919E714C34850B408AFCD746D7"
    },
    "cell_type" : "code",
    "source" : "classify(\"het regent vandaag\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab544806057-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "27DE7E84356D49AB9D0E4BF9B30E4777"
    },
    "cell_type" : "code",
    "source" : "classify(\"language\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1670149214-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "A7746E9B9DB24A3CB21E9BBA0A4CCDA3"
    },
    "cell_type" : "code",
    "source" : "classify(\"these little sentences\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab433869420-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E1F0E3038747454B8FC476C9F20AEBB8"
    },
    "cell_type" : "code",
    "source" : "classify(\"are too small\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A30259B131B241D39A18EFABF8F1E9A3"
    },
    "cell_type" : "markdown",
    "source" : "#Let's evaluate our classification model"
  }, {
    "metadata" : {
      "id" : "96C943D6FBBB4757A995A642EF5B2C8F"
    },
    "cell_type" : "markdown",
    "source" : "## We need a test dataset"
  }, {
    "metadata" : {
      "id" : "DDD38FD5736C4E7080527B4C611DE253"
    },
    "cell_type" : "markdown",
    "source" : "We want to see how our model performs with text of different lengths. We'll randomly sample the original text in order to create a labeled sample. \n\nNote that we are labeling the samples with the actual size."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3C2764EC04334DB2880BFB48B85C638D"
    },
    "cell_type" : "code",
    "source" : "val sampleDataByLanguage = langDataset.map{case (lang, rdd) => \n                val sample = rdd.sample(withReplacement = false, fraction = 0.3)\n                                           .map(str => str.collect{case char if Character.isAlphabetic(char) || Character.isWhitespace(char) => Character.toLowerCase(char)})\n                sample.map{str => \n                        val sentenceLength = scala.util.Random.nextInt(100)+1\n                        val sampled = str.take(sentenceLength).trim\n                        (lang, sampled.size, sampled)\n                          }.filter{case (_, _, sampled) =>  sampled.size > 0 }\n                }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "23FB6A4DA52F421A810DA2F40FE95211"
    },
    "cell_type" : "markdown",
    "source" : "### Here we union all the samples from the different languages in a consolidated test dataset\nThen we make a dataframe in order to easy the slicing and dicing we're going to perform next"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "63DA0724BCB445BB88F6E4ED9870D1EC"
    },
    "cell_type" : "code",
    "source" : "val testDataset = sparkContext.union(sampleDataByLanguage).coalesce(8).toDF(\"language\", \"textSize\", \"text\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E5D6B059731B4233B866337879946ED6"
    },
    "cell_type" : "code",
    "source" : "testDataset.sample(withReplacement = false, fraction = 0.0005)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6900F4E571094472A152E0575EC155C3"
    },
    "cell_type" : "code",
    "source" : "<h3>Our test dataset contains {testDataset.count()} records</h3>",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "C16E50D6CF32498F80F0E80A34D76D8B"
    },
    "cell_type" : "markdown",
    "source" : "#### We can easily use shell functions :-)\nIn this case, we clean up the target save dir to avoid issues when writing our dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "06E7C327B8EB46D58FFFF77ACD2F7E7C"
    },
    "cell_type" : "code",
    "source" : ":sh rm -rf /tmp/testDataset",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D21391A771A84B74AEBD0F35174C98A9"
    },
    "cell_type" : "markdown",
    "source" : "### We save our test dataset for potential use later on..."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "17DE60116B3441F5A810DBE5C311C7A1"
    },
    "cell_type" : "code",
    "source" : "testDataset.write.mode(SaveMode.Overwrite).parquet(\"/tmp/testDataset\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4B2DEFD0B4B04C2E89C82A3A6E566568"
    },
    "cell_type" : "markdown",
    "source" : "### Lets do some sanity checks on out test data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab89721165-2\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E2F623D45F914253BA2017D52E5948DB"
    },
    "cell_type" : "code",
    "source" : "testDataset.select($\"language\", $\"textSize\").groupBy($\"textSize\").count().orderBy($\"textSize\").collect()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C7FA096B691745908DAF2989C95E7032"
    },
    "cell_type" : "code",
    "source" : "testDataset.select($\"text\").where($\"textSize\" === 7)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BF18914EE07242649E6A6D534663D7EE"
    },
    "cell_type" : "markdown",
    "source" : "### Use an UDF to apply the predict function to our dataset\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B01DFDE685CC448A80F700BE22EED85E"
    },
    "cell_type" : "code",
    "source" : "val classifyUDF = {\n  val func: String => String = str => classify(str).head._1\n  udf(func)\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3C577ACBAE1847CBB9CD1ADE845F14BD"
    },
    "cell_type" : "markdown",
    "source" : "### And we apply the classifier to the test dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D50DD2D023FF46E0AFB93BC42C9D4528"
    },
    "cell_type" : "code",
    "source" : "val hitAndMiss = testDataset.withColumn(\"lang_classification\", classifyUDF($\"text\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "E2C948615EF84F278F5DC32F115AEFD7"
    },
    "cell_type" : "markdown",
    "source" : "#### Let's see some of the results"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1271576044-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "76D1FED1200B4DF18E0C96301B4A02E1"
    },
    "cell_type" : "code",
    "source" : "hitAndMiss.sample(false, 0.001).collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "FB51DCBC56E0403FAFE43F1D263178C1"
    },
    "cell_type" : "markdown",
    "source" : "## Let's evaluate the overall hit ratio\nAKA Model Accuracy"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AC749C8104F8439D9375F975288F412C"
    },
    "cell_type" : "code",
    "source" : "val hitCol = when($\"language\" === $\"lang_classification\", 1L).otherwise(0L)\nval hits = hitAndMiss.withColumn(\"hit\", hitCol).withColumn(\"counter\", lit(1))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "305662E192844B5BBB621DED26100CF4"
    },
    "cell_type" : "code",
    "source" : "val hitRatio = hits.select(sum($\"hit\")/sum($\"counter\")).head.getDouble(0)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "17FBE2383F5549F396EEF0A97170E11A"
    },
    "cell_type" : "markdown",
    "source" : "## We learnt that our method performs bad on small texts\n### How is the performance in relation to text length (measured in words, not characters)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5A8E8364FBE644CEAF55AE4FEC3CA126"
    },
    "cell_type" : "code",
    "source" : "val hitRatioPerLength = hits.groupBy($\"textSize\").agg(sum($\"hit\")/sum($\"counter\")).orderBy($\"textSize\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "0B9C8D2BC336462BB30072272231EB1E"
    },
    "cell_type" : "markdown",
    "source" : "#Model accuracy vs sentence length"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab187936961-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "9F7DA76D76C74FF4BE8D6C3E02B99776"
    },
    "cell_type" : "code",
    "source" : "val ds = hitRatioPerLength.as[(Long, Double)]",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1857371598-2\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "6E3C23EFF56F46E78C21B7AE89C6ED70"
    },
    "cell_type" : "code",
    "source" : "ds.collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "2140F8A946F243E7A0A77C2DE9D0E541"
    },
    "cell_type" : "markdown",
    "source" : "#=o="
  } ],
  "nbformat" : 4
}